{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from model import *\n",
    "import tensorflow as tf\n",
    "import setGPU\n",
    "#strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n",
    "\n",
    "imgs = np.load('../dataset/cv/train/x/imgs.npz')['arr_0']\n",
    "labels = np.load('../dataset/cv/train/y/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = set(char for label in labels for char in label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=sorted(list(characters)), num_oov_indices=0, mask_token=None )\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.transpose(imgs, (0, 2, 1, 3))\n",
    "'''\n",
    "max_len = max([len(label) for label in labels])\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label:\n",
    "        tmp = np.array([char_to_num(char) for char in label])\n",
    "        encoded_labels.append(np.pad(tmp, (0, max_len-tmp.shape[0]), constant_values=0))\n",
    "    else:\n",
    "        tmp = np.array([char_to_num('#')])\n",
    "        encoded_labels.append(np.pad(tmp, (0, max_len-tmp.shape[0]), constant_values=0))\n",
    "'''\n",
    "\n",
    "encoded_labels = np.load('../dataset/cv/train/y/encoded_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si inserisce alla fine di ogni array label la lunghezza reale della parola\n",
    "t_dist_dim = int(128)          # Questo valore indica quanti step temporali ci sono: nelle ultime feature map ci sono 128/4\n",
    "                                 # step temporali perché 128 è la larghezza massima tra le immagini in ingresso e 4 è il fattore\n",
    "                                 # di riduzione dovuto ai MaxPooling (ci sono 2 livelli di MaxPooling che dimezzano le dimensioni)\n",
    "enc2 = []\n",
    "for i in range(encoded_labels.shape[0]):\n",
    "    if len(labels[i]) == 0:\n",
    "        enc2.append(np.append(encoded_labels[i], [1, t_dist_dim]))\n",
    "    else:\n",
    "        enc2.append(np.append(encoded_labels[i], [len(labels[i]), t_dist_dim]))\n",
    "enc2 = np.array(enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "net = build_and_compile_model_v6(input_shape=(128, 128, 3), len_characters=len(characters), opt=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 64, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 64, 128)      73856     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 64, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 64, 256)      295168    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 64, 256)      590080    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128, 64, 256)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 32, 256)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 32, 512)      1180160   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 32, 512)      2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 128, 16, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 16, 512)      1049088   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128, 16, 512)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 128, 8192)         0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128, 64)           524352    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128, 256)          197632    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128, 256)          394240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128, 257)          66049     \n",
      "=================================================================\n",
      "Total params: 4,374,465\n",
      "Trainable params: 4,373,441\n",
      "Non-trainable params: 1,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "Early_Stopping_Patience = 10 \n",
    "Min_Delta = 0.0001\n",
    "# Model Check Point\n",
    "Check_Point = ModelCheckpoint( 'weights/CRNN_v6.h5',   # Filepath\n",
    "                               monitor='val_loss',\n",
    "                               save_best_only=True,\n",
    "                               verbose=1,\n",
    "                               mode='auto',\n",
    "                               save_weights_only=False,\n",
    "                               save_freq='epoch' )\n",
    "# Add early stopping\n",
    "Early_Stopping = EarlyStopping( monitor='val_loss',\n",
    "                                min_delta=Min_Delta,\n",
    "                                patience=Early_Stopping_Patience,\n",
    "                                verbose=1,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True )\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "x_train = imgs\n",
    "y_train = enc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  2/906 [..............................] - ETA: 1:44 - loss: 610.6964WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0883s vs `on_train_batch_end` time: 0.1403s). Check your callbacks.\n",
      "119/906 [==>...........................] - ETA: 2:58 - loss: 44.4136"
     ]
    }
   ],
   "source": [
    "history = net.fit(x_train, y_train, validation_split=0.2, epochs=100, callbacks=[Check_Point, reduce_lr], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('my_history_v4.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [29.55524253845215,\n",
       "  23.871219635009766,\n",
       "  22.930871963500977,\n",
       "  22.60465431213379,\n",
       "  22.413803100585938,\n",
       "  22.198286056518555,\n",
       "  22.000946044921875,\n",
       "  21.785634994506836,\n",
       "  21.498363494873047,\n",
       "  21.11525535583496,\n",
       "  20.771413803100586,\n",
       "  20.312236785888672,\n",
       "  19.845294952392578,\n",
       "  19.25650405883789,\n",
       "  18.68903350830078,\n",
       "  18.20015525817871,\n",
       "  17.712413787841797,\n",
       "  17.3137264251709,\n",
       "  17.127174377441406,\n",
       "  17.009464263916016,\n",
       "  16.68659782409668,\n",
       "  16.430591583251953,\n",
       "  16.270002365112305,\n",
       "  16.138019561767578,\n",
       "  15.820940971374512,\n",
       "  15.532114028930664,\n",
       "  15.313992500305176,\n",
       "  16.10277557373047,\n",
       "  15.31989574432373,\n",
       "  15.262053489685059,\n",
       "  15.380794525146484,\n",
       "  15.673229217529297,\n",
       "  15.006155014038086,\n",
       "  14.855964660644531,\n",
       "  14.804557800292969,\n",
       "  14.627645492553711,\n",
       "  14.880324363708496,\n",
       "  14.981422424316406,\n",
       "  15.629707336425781,\n",
       "  16.130992889404297,\n",
       "  15.34249210357666,\n",
       "  14.98897933959961,\n",
       "  14.976032257080078,\n",
       "  14.305858612060547,\n",
       "  14.606742858886719,\n",
       "  14.521474838256836,\n",
       "  14.556227684020996,\n",
       "  14.593901634216309,\n",
       "  14.504677772521973,\n",
       "  14.76473331451416,\n",
       "  14.48523235321045,\n",
       "  13.963118553161621,\n",
       "  14.116785049438477,\n",
       "  14.13471794128418,\n",
       "  14.171478271484375,\n",
       "  13.171634674072266,\n",
       "  13.06483268737793,\n",
       "  12.96107006072998,\n",
       "  12.908321380615234,\n",
       "  12.901439666748047,\n",
       "  12.903921127319336,\n",
       "  12.771547317504883,\n",
       "  12.721661567687988,\n",
       "  12.863015174865723,\n",
       "  12.697761535644531,\n",
       "  12.610149383544922,\n",
       "  12.585061073303223,\n",
       "  12.68986701965332,\n",
       "  12.515271186828613,\n",
       "  12.448768615722656,\n",
       "  12.472465515136719,\n",
       "  12.380256652832031,\n",
       "  12.335036277770996,\n",
       "  12.314301490783691,\n",
       "  12.41089916229248,\n",
       "  12.281813621520996,\n",
       "  12.357402801513672,\n",
       "  12.466470718383789,\n",
       "  12.266397476196289,\n",
       "  12.224398612976074,\n",
       "  12.112055778503418,\n",
       "  12.097552299499512,\n",
       "  12.079432487487793,\n",
       "  12.000265121459961,\n",
       "  12.008337020874023,\n",
       "  12.151811599731445,\n",
       "  12.070647239685059,\n",
       "  12.071455955505371,\n",
       "  12.080046653747559,\n",
       "  11.99148941040039,\n",
       "  12.06452751159668,\n",
       "  11.957813262939453,\n",
       "  11.934599876403809,\n",
       "  12.012445449829102,\n",
       "  12.053293228149414,\n",
       "  12.089194297790527,\n",
       "  11.928451538085938,\n",
       "  11.929025650024414,\n",
       "  11.9193754196167,\n",
       "  12.019968032836914],\n",
       " 'val_loss': [24.932701110839844,\n",
       "  23.873964309692383,\n",
       "  23.628862380981445,\n",
       "  22.861995697021484,\n",
       "  22.862136840820312,\n",
       "  22.41440200805664,\n",
       "  22.520801544189453,\n",
       "  22.193614959716797,\n",
       "  21.556764602661133,\n",
       "  21.455867767333984,\n",
       "  20.808069229125977,\n",
       "  20.831398010253906,\n",
       "  19.96340560913086,\n",
       "  19.438451766967773,\n",
       "  19.144432067871094,\n",
       "  18.045265197753906,\n",
       "  18.40019416809082,\n",
       "  17.77118492126465,\n",
       "  17.357927322387695,\n",
       "  19.398284912109375,\n",
       "  16.992746353149414,\n",
       "  16.348140716552734,\n",
       "  16.527997970581055,\n",
       "  16.114595413208008,\n",
       "  15.800312042236328,\n",
       "  16.473630905151367,\n",
       "  15.551332473754883,\n",
       "  15.662779808044434,\n",
       "  15.251151084899902,\n",
       "  15.521296501159668,\n",
       "  19.58966827392578,\n",
       "  15.318169593811035,\n",
       "  14.728784561157227,\n",
       "  14.824904441833496,\n",
       "  15.037332534790039,\n",
       "  14.627887725830078,\n",
       "  14.735844612121582,\n",
       "  16.020702362060547,\n",
       "  15.535176277160645,\n",
       "  17.39261817932129,\n",
       "  15.091453552246094,\n",
       "  14.870981216430664,\n",
       "  14.649344444274902,\n",
       "  14.464399337768555,\n",
       "  15.654831886291504,\n",
       "  14.885228157043457,\n",
       "  14.794610023498535,\n",
       "  15.609522819519043,\n",
       "  17.34011459350586,\n",
       "  15.806045532226562,\n",
       "  14.620198249816895,\n",
       "  14.826190948486328,\n",
       "  14.648653984069824,\n",
       "  18.30742645263672,\n",
       "  13.557866096496582,\n",
       "  13.292411804199219,\n",
       "  13.252217292785645,\n",
       "  13.216283798217773,\n",
       "  13.192967414855957,\n",
       "  13.099993705749512,\n",
       "  13.13327693939209,\n",
       "  13.061849594116211,\n",
       "  13.01679801940918,\n",
       "  13.195013046264648,\n",
       "  12.916847229003906,\n",
       "  12.955307960510254,\n",
       "  13.034955978393555,\n",
       "  13.113424301147461,\n",
       "  12.785394668579102,\n",
       "  12.762056350708008,\n",
       "  12.950111389160156,\n",
       "  12.690946578979492,\n",
       "  12.712817192077637,\n",
       "  12.649374961853027,\n",
       "  12.843720436096191,\n",
       "  12.700844764709473,\n",
       "  13.504103660583496,\n",
       "  12.625526428222656,\n",
       "  12.741365432739258,\n",
       "  12.545735359191895,\n",
       "  12.563952445983887,\n",
       "  12.547789573669434,\n",
       "  12.48934268951416,\n",
       "  12.431821823120117,\n",
       "  12.510326385498047,\n",
       "  12.630452156066895,\n",
       "  12.516162872314453,\n",
       "  12.524773597717285,\n",
       "  12.501069068908691,\n",
       "  12.47935962677002,\n",
       "  12.560162544250488,\n",
       "  12.40068531036377,\n",
       "  12.443878173828125,\n",
       "  12.420435905456543,\n",
       "  12.605161666870117,\n",
       "  12.599508285522461,\n",
       "  12.508997917175293,\n",
       "  12.278944969177246,\n",
       "  12.304696083068848,\n",
       "  12.535904884338379],\n",
       " 'lr': [0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001,\n",
       "  0.00020000001]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
