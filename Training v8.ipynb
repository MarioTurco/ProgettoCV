{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setGPU: Setting GPU to: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from model import *\n",
    "import tensorflow as tf\n",
    "import setGPU\n",
    "#strategy = tf.distribute.MirroredStrategy([\"GPU:0\", \"GPU:1\"])\n",
    "\n",
    "imgs = np.load('../dataset/cv/train/x/imgs_grayscale.npz')['arr_0']\n",
    "labels = np.load('../dataset/cv/train/y/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = set(char for label in labels for char in label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping characters to integers\n",
    "char_to_num = layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=sorted(list(characters)), num_oov_indices=0, mask_token=None )\n",
    "\n",
    "# Mapping integers back to original characters\n",
    "num_to_char = layers.experimental.preprocessing.StringLookup(\n",
    "                  vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.transpose(imgs, (0, 2, 1, 3))\n",
    "'''\n",
    "max_len = max([len(label) for label in labels])\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    if label:\n",
    "        tmp = np.array([char_to_num(char) for char in label])\n",
    "        encoded_labels.append(np.pad(tmp, (0, max_len-tmp.shape[0]), constant_values=0))\n",
    "    else:\n",
    "        tmp = np.array([char_to_num('#')])\n",
    "        encoded_labels.append(np.pad(tmp, (0, max_len-tmp.shape[0]), constant_values=0))\n",
    "'''\n",
    "\n",
    "encoded_labels = np.load('../dataset/cv/train/y/encoded_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si inserisce alla fine di ogni array label la lunghezza reale della parola\n",
    "t_dist_dim = int(128)          # Questo valore indica quanti step temporali ci sono: nelle ultime feature map ci sono 128/4\n",
    "                                 # step temporali perché 128 è la larghezza massima tra le immagini in ingresso e 4 è il fattore\n",
    "                                 # di riduzione dovuto ai MaxPooling (ci sono 2 livelli di MaxPooling che dimezzano le dimensioni)\n",
    "enc2 = []\n",
    "for i in range(encoded_labels.shape[0]):\n",
    "    if len(labels[i]) == 0:\n",
    "        enc2.append(np.append(encoded_labels[i], [1, t_dist_dim]))\n",
    "    else:\n",
    "        enc2.append(np.append(encoded_labels[i], [len(labels[i]), t_dist_dim]))\n",
    "enc2 = np.array(enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with strategy.scope():\n",
    "net = build_and_compile_model_v8(input_shape=(128, 128, 1), len_characters=len(characters), opt=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128, 128, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 128, 128, 64)      640       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 64, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 64, 128)      73856     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 64, 128)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 128, 32, 128)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 128, 32, 256)      295168    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 32, 256)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 32, 256)      1024      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 128, 32, 512)      1180160   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 128, 32, 512)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 16, 512)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 128, 16, 512)      1049088   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128, 16, 512)      0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 128, 8192)         0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128, 64)           524352    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128, 512)          657408    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128, 512)          1574912   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128, 257)          131841    \n",
      "=================================================================\n",
      "Total params: 5,488,449\n",
      "Trainable params: 5,487,937\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "Early_Stopping_Patience = 10 \n",
    "Min_Delta = 0.0001\n",
    "# Model Check Point\n",
    "Check_Point = ModelCheckpoint( 'weights/CRNN_v8.h5',   # Filepath\n",
    "                               monitor='val_loss',\n",
    "                               save_best_only=True,\n",
    "                               verbose=1,\n",
    "                               mode='auto',\n",
    "                               save_weights_only=False,\n",
    "                               save_freq='epoch' )\n",
    "# Add early stopping\n",
    "Early_Stopping = EarlyStopping( monitor='val_loss',\n",
    "                                min_delta=Min_Delta,\n",
    "                                patience=Early_Stopping_Patience,\n",
    "                                verbose=1,\n",
    "                                mode='auto',\n",
    "                                baseline=None,\n",
    "                                restore_best_weights=True )\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
    "                              patience=10, min_lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "x_train = imgs\n",
    "y_train = enc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 25.5380\n",
      "Epoch 00001: val_loss improved from inf to 24.79182, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 183s 202ms/step - loss: 25.5380 - val_loss: 24.7918\n",
      "Epoch 2/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 23.3246\n",
      "Epoch 00002: val_loss improved from 24.79182 to 24.63578, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 181s 200ms/step - loss: 23.3246 - val_loss: 24.6358\n",
      "Epoch 3/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 22.7946\n",
      "Epoch 00003: val_loss did not improve from 24.63578\n",
      "906/906 [==============================] - 181s 200ms/step - loss: 22.7946 - val_loss: 25.0262\n",
      "Epoch 4/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 22.3069\n",
      "Epoch 00004: val_loss improved from 24.63578 to 22.79808, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 181s 200ms/step - loss: 22.3069 - val_loss: 22.7981\n",
      "Epoch 5/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 22.0258\n",
      "Epoch 00005: val_loss improved from 22.79808 to 22.44464, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 181s 200ms/step - loss: 22.0258 - val_loss: 22.4446\n",
      "Epoch 6/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.8127\n",
      "Epoch 00006: val_loss did not improve from 22.44464\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.8127 - val_loss: 23.3048\n",
      "Epoch 7/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.6354\n",
      "Epoch 00007: val_loss did not improve from 22.44464\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.6354 - val_loss: 22.5728\n",
      "Epoch 8/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.5434\n",
      "Epoch 00008: val_loss improved from 22.44464 to 22.13464, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.5434 - val_loss: 22.1346\n",
      "Epoch 9/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.4788\n",
      "Epoch 00009: val_loss did not improve from 22.13464\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.4788 - val_loss: 22.2531\n",
      "Epoch 10/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.4667\n",
      "Epoch 00010: val_loss did not improve from 22.13464\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.4667 - val_loss: 22.1743\n",
      "Epoch 11/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.3053\n",
      "Epoch 00011: val_loss improved from 22.13464 to 22.06974, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.3053 - val_loss: 22.0697\n",
      "Epoch 12/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.2882\n",
      "Epoch 00012: val_loss did not improve from 22.06974\n",
      "906/906 [==============================] - 180s 198ms/step - loss: 21.2882 - val_loss: 24.2083\n",
      "Epoch 13/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.1492\n",
      "Epoch 00013: val_loss improved from 22.06974 to 21.97466, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.1492 - val_loss: 21.9747\n",
      "Epoch 14/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.2634\n",
      "Epoch 00014: val_loss did not improve from 21.97466\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.2634 - val_loss: 22.0227\n",
      "Epoch 15/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.1129\n",
      "Epoch 00015: val_loss did not improve from 21.97466\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.1129 - val_loss: 22.1337\n",
      "Epoch 16/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.1272\n",
      "Epoch 00016: val_loss did not improve from 21.97466\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.1272 - val_loss: 22.3560\n",
      "Epoch 17/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.0775\n",
      "Epoch 00017: val_loss improved from 21.97466 to 21.91926, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.0775 - val_loss: 21.9193\n",
      "Epoch 18/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.1233\n",
      "Epoch 00018: val_loss did not improve from 21.91926\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.1233 - val_loss: 22.1026\n",
      "Epoch 19/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.9319\n",
      "Epoch 00019: val_loss improved from 21.91926 to 21.82459, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 181s 199ms/step - loss: 20.9319 - val_loss: 21.8246\n",
      "Epoch 20/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.1803\n",
      "Epoch 00020: val_loss did not improve from 21.82459\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.1803 - val_loss: 22.7914\n",
      "Epoch 21/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.0154\n",
      "Epoch 00021: val_loss did not improve from 21.82459\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 21.0154 - val_loss: 21.9047\n",
      "Epoch 22/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.9007\n",
      "Epoch 00022: val_loss did not improve from 21.82459\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 20.9007 - val_loss: 22.2095\n",
      "Epoch 23/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.9294\n",
      "Epoch 00023: val_loss did not improve from 21.82459\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 20.9294 - val_loss: 21.9145\n",
      "Epoch 24/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.9465\n",
      "Epoch 00024: val_loss improved from 21.82459 to 21.80618, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 180s 198ms/step - loss: 20.9465 - val_loss: 21.8062\n",
      "Epoch 25/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.9185\n",
      "Epoch 00025: val_loss did not improve from 21.80618\n",
      "906/906 [==============================] - 178s 197ms/step - loss: 20.9185 - val_loss: 23.1216\n",
      "Epoch 26/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.9837\n",
      "Epoch 00026: val_loss did not improve from 21.80618\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.9837 - val_loss: 21.9362\n",
      "Epoch 27/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 21.2717\n",
      "Epoch 00027: val_loss did not improve from 21.80618\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 21.2717 - val_loss: 22.5314\n",
      "Epoch 28/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.8745\n",
      "Epoch 00028: val_loss did not improve from 21.80618\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.8745 - val_loss: 22.1379\n",
      "Epoch 29/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.7609\n",
      "Epoch 00029: val_loss improved from 21.80618 to 21.63078, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.7609 - val_loss: 21.6308\n",
      "Epoch 30/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.7495\n",
      "Epoch 00030: val_loss did not improve from 21.63078\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.7495 - val_loss: 22.4482\n",
      "Epoch 31/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.6220\n",
      "Epoch 00031: val_loss improved from 21.63078 to 21.43756, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.6220 - val_loss: 21.4376\n",
      "Epoch 32/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.6306\n",
      "Epoch 00032: val_loss did not improve from 21.43756\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.6306 - val_loss: 21.6381\n",
      "Epoch 33/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.5825\n",
      "Epoch 00033: val_loss did not improve from 21.43756\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.5825 - val_loss: 21.6988\n",
      "Epoch 34/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.7594\n",
      "Epoch 00034: val_loss did not improve from 21.43756\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.7594 - val_loss: 22.1942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.5931\n",
      "Epoch 00035: val_loss did not improve from 21.43756\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.5931 - val_loss: 21.7319\n",
      "Epoch 36/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.5117\n",
      "Epoch 00036: val_loss did not improve from 21.43756\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.5117 - val_loss: 22.0106\n",
      "Epoch 37/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.4912\n",
      "Epoch 00037: val_loss improved from 21.43756 to 21.42988, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.4912 - val_loss: 21.4299\n",
      "Epoch 38/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.5510\n",
      "Epoch 00038: val_loss did not improve from 21.42988\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.5510 - val_loss: 22.4527\n",
      "Epoch 39/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.4725\n",
      "Epoch 00039: val_loss improved from 21.42988 to 21.38118, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.4725 - val_loss: 21.3812\n",
      "Epoch 40/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.3804\n",
      "Epoch 00040: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.3804 - val_loss: 21.9288\n",
      "Epoch 41/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.3783\n",
      "Epoch 00041: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.3783 - val_loss: 21.4908\n",
      "Epoch 42/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.3786\n",
      "Epoch 00042: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.3786 - val_loss: 22.3814\n",
      "Epoch 43/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.2557\n",
      "Epoch 00043: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.2557 - val_loss: 22.0258\n",
      "Epoch 44/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.6019\n",
      "Epoch 00044: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.6019 - val_loss: 21.5467\n",
      "Epoch 45/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.4812\n",
      "Epoch 00045: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.4812 - val_loss: 21.8280\n",
      "Epoch 46/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.3908\n",
      "Epoch 00046: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.3908 - val_loss: 24.5798\n",
      "Epoch 47/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.2858\n",
      "Epoch 00047: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.2858 - val_loss: 21.5943\n",
      "Epoch 48/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.4609\n",
      "Epoch 00048: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.4609 - val_loss: 22.7394\n",
      "Epoch 49/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.8561\n",
      "Epoch 00049: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 20.8561 - val_loss: 22.8645\n",
      "Epoch 50/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.7666\n",
      "Epoch 00050: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.7666 - val_loss: 21.4996\n",
      "Epoch 51/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.3141\n",
      "Epoch 00051: val_loss did not improve from 21.38118\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 20.3141 - val_loss: 21.7560\n",
      "Epoch 52/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 20.2050\n",
      "Epoch 00052: val_loss improved from 21.38118 to 21.18469, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 180s 198ms/step - loss: 20.2050 - val_loss: 21.1847\n",
      "Epoch 53/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.9211\n",
      "Epoch 00053: val_loss improved from 21.18469 to 21.12846, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 181s 199ms/step - loss: 19.9211 - val_loss: 21.1285\n",
      "Epoch 54/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.8333\n",
      "Epoch 00054: val_loss improved from 21.12846 to 21.07734, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 19.8333 - val_loss: 21.0773\n",
      "Epoch 55/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.7820\n",
      "Epoch 00055: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 19.7820 - val_loss: 21.0851\n",
      "Epoch 56/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.6997\n",
      "Epoch 00056: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 181s 199ms/step - loss: 19.6997 - val_loss: 21.1122\n",
      "Epoch 57/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.6789\n",
      "Epoch 00057: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 19.6789 - val_loss: 21.0980\n",
      "Epoch 58/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.6371\n",
      "Epoch 00058: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 19.6371 - val_loss: 21.2773\n",
      "Epoch 59/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.6775\n",
      "Epoch 00059: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 19.6775 - val_loss: 21.1842\n",
      "Epoch 60/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.6464\n",
      "Epoch 00060: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 179s 197ms/step - loss: 19.6464 - val_loss: 21.2375\n",
      "Epoch 61/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.6088\n",
      "Epoch 00061: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 19.6088 - val_loss: 21.2604\n",
      "Epoch 62/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.5622\n",
      "Epoch 00062: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 19.5622 - val_loss: 21.1152\n",
      "Epoch 63/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.5112\n",
      "Epoch 00063: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 179s 198ms/step - loss: 19.5112 - val_loss: 21.2123\n",
      "Epoch 64/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.4739\n",
      "Epoch 00064: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 180s 199ms/step - loss: 19.4739 - val_loss: 21.1863\n",
      "Epoch 65/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.4348\n",
      "Epoch 00065: val_loss did not improve from 21.07734\n",
      "906/906 [==============================] - 181s 199ms/step - loss: 19.4348 - val_loss: 21.1454\n",
      "Epoch 66/100\n",
      "906/906 [==============================] - ETA: 0s - loss: 19.3520\n",
      "Epoch 00066: val_loss improved from 21.07734 to 21.01919, saving model to weights/CRNN_v8.h5\n",
      "906/906 [==============================] - 181s 199ms/step - loss: 19.3520 - val_loss: 21.0192\n",
      "Epoch 67/100\n",
      "501/906 [===============>..............] - ETA: 1:13 - loss: 19.1462"
     ]
    }
   ],
   "source": [
    "history = net.fit(x_train, y_train, validation_split=0.2, epochs=100, callbacks=[Check_Point, reduce_lr], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('history/my_history_v8.npy',history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7e503554f0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(1, 101), history.history['loss'])\n",
    "plt.plot(range(1, 101), history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_to_char_dict = {0: '0',\n",
    "                        1: '1',\n",
    "                        2: '2',\n",
    "                        3: '3',\n",
    "                        4: '4',\n",
    "                        5: '5',\n",
    "                        6: '6',\n",
    "                        7: '7',\n",
    "                        8: '8',\n",
    "                        9: '9'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ctc(args):\n",
    "    \"\"\"returns a list of decoded ctc losses\"\"\"\n",
    "\n",
    "    y_pred, input_length = args\n",
    "\n",
    "    ctc_decoded = tf.keras.backend.ctc_decode(\n",
    "        y_pred, input_length, greedy=True)\n",
    "\n",
    "    return ctc_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 257)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(np.expand_dims(imgs[0], axis=0)).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/grauso/.local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
     ]
    }
   ],
   "source": [
    "pred_tensor, _ = decode_ctc([net.predict(np.expand_dims(imgs[0], axis=0)), np.array([t_dist_dim])])\n",
    "pred_labels = tf.keras.backend.get_value(pred_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35, 49, 50, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
